# Partie 4 - Crawling et scraping des pages

* ‚è∞ Dur√©e : 25min
* üéØ Objectifs :
  * Cr√©ation de l‚Äôactivit√© de Scraping qui depuis une source avec URI va t√©l√©charger la page.
  * Appeler l'activit√© de crawling en amont
  * Comprendre le concept de Continue As New

<!-- TOC -->
* [Partie 4 - Crawling et scraping des pages](#partie-4---crawling-et-scraping-des-pages)
  * [Cr√©ation de l'activit√© de Scraping](#cr√©ation-de-lactivit√©-de-scraping)
  * [Appel de l'activit√© de Crawling](#appel-de-lactivit√©-de-crawling)
  * [Tester l'ensemble](#tester-lensemble)
<!-- TOC -->

## Cr√©ation de l'activit√© de Scraping

Ajoutons d'abord une activit√© qui depuis l'URL d'une page web dont on sait qu'il s'agit d'une page HTML (mime_type: text/html)
est capable la t√©l√©charger dans le dossier de travail temporal.

![](./images/workflow-pipeline-part4.excalidraw.png)

Arborescence du dossier que nous souhaitons avoir :
```
temporal_workdir/
‚îÇ
‚îî‚îÄ‚îÄ {uuid-workflow}/
    ‚îÇ
    ‚îú‚îÄ‚îÄ scraping/                 # Pages HTML brutes r√©cup√©r√©es, en fin de stage de scraping
    ‚îÇ   ‚îú‚îÄ‚îÄ brest.fr_actualites_palaren-un-belvedere-sur-rade-en-preparation.html
    ‚îÇ   ‚îî‚îÄ‚îÄ brest.fr_actualites_debut-des-travaux-pour-les-ascenseurs-inclines-du-chu.html
    ‚îÇ
    ‚îî‚îÄ‚îÄ conversion/               # Fichiers Markdown convertis, en fin de stage de convertion
        ‚îú‚îÄ‚îÄ brest.fr_actualites_palaren-un-belvedere-sur-rade-en-preparation.html.md
        ‚îî‚îÄ‚îÄ brest.fr_actualites_debut-des-travaux-pour-les-ascenseurs-inclines-du-chu.html.md
```

Vous pourrez utiliser la librairie requests pour t√©l√©charger les fichiers a vous de jouer et de compl√©ter le fichier
[scraping_activity.py](../avelbot-ingestion-py/src/avelbot_ingestion/activities/scraping_activity.py) en vous inspirant
par exemple de [print_source_activity.py](../avelbot-ingestion-py/src/avelbot_ingestion/activities/print_source_activity.py).

Le cadre et le suivant :
* L'activit√© s'appelle `PY-scraping_activity`.
* Elle prend en entr√©e une source.
* Elle doit s'assurer que le dossier `scraping` existe en sachant que le worker s'ex√©cute depuis la racine du projet git,
donc que `./temporal_workdir` pointe bien vers le dossier de travail de Temporal.
  * Vous pouvez acc√©der √† l'UUID du workflow courant en utilisant
```python
from temporalio import activity

workflow_id = activity.info().workflow_id
```
* V√©rifier que la source est bien de type `MimeTypesEnum.text_html`, sinon la marquer en erreur.
* Vous pouvez utiliser requests pour t√©l√©charger en GET les sources.
* Vous pouvez vous aider de TODO fonction utilitaire pour g√©n√©rer un nom normalis√© depuis une URI :
```python
from avelbot_ingestion.helpers.url_helpers import url_to_file_name

url_to_file_name(source.uri)
```
* ‚ö† N'oubliez pas toutes nouvelle activit√© doit √™tre r√©f√©renc√©e sur un worker üòâ
* TODO Ajouter au workflow l'appel vers cette activit√©
* Vous pouvez tester √† l'aide de la run configuration :
  * PyCharm `Part 4.a - Scraping`
  * VS Code `[üêç] Trigger - Part 4.a - Scraping`

Si besoin la solution ci-dessous ‚¨áÔ∏è.

<details>
  <summary>Solution scraping_activity.py</summary>

```python
# COMPLETER ICI - START (partie 4)
import os
from temporalio import activity

from avelbot_ingestion.helpers.logging_config import get_app_logger
from avelbot_ingestion.helpers.url_helpers import url_to_file_name
from avelbot_ingestion.models.MimeTypesEnum import MimeTypesEnum
from avelbot_ingestion.models.Source import Source

import requests
from pathlib import Path

from avelbot_ingestion.models.StageEnum import StageEnum

logger = get_app_logger(__name__)

@activity.defn(name="PY-scraping_activity")
async def scraping_activity(source: Source) -> Source:
    """
    Activit√©e qui t√©l√©charge une page web.

    :param source: Source a t√©l√©charger.
    """
    logger.info(" ---- scraping_activity for %s ----", source.uri)

    if source.mime_type != MimeTypesEnum.text_html:
        source.error = "Not and HTML mime type, can't be scraped."
        return source

    source.current_stage = StageEnum.SCRAPING

    # Ensure output directory exists
    workflow_id = activity.info().workflow_id
    scraping_directory = Path("temporal_workdir") / workflow_id / "scraping"
    os.makedirs(scraping_directory, exist_ok=True)
    output_raw_file_path = Path(scraping_directory) / f"{url_to_file_name(source.uri)}.html"

    # Download page
    try:
        resp = requests.get(source.uri, timeout=10)
        resp.raise_for_status()

        output_raw_file_path.write_text(resp.text, encoding="utf-8")
        source.raw_file_path = str(output_raw_file_path)
    except Exception as e:
        source.error = f"Failed to download page: {e}"

    return  source

# COMPLETER ICI - END (partie 4)
```
</details>

## Appel de l'activit√© de Crawling

L'activit√© de crawling est d√©j√† d√©velopp√©e dans [crawling_activity.py](../avelbot-ingestion-py/src/avelbot_ingestion/activities/crawling_activity.py).

Son fonctionnement est assez simple, elle prend en entr√©e une source et cr√©√© un source pour chauque lien trouv√© dans
la page correspond dont l'url commence par un des motifs d√©finit en option de source `source.options.crawling_url_startswith`.

Je vous invite √† voir le fichier [part4.b-crawl.yml](../avelbot-ingestion-py/ressources/workflow_inputs/part4.b-crawl.yml),
qui d√©crit ces la config d'entr√©e du workflow que nous allons utiliser.

Pour activer ce crawling vous pouvez simplement d√©commenter le block suivant dans
[ingestion_workflow.py](../avelbot-ingestion-py/src/avelbot_ingestion/workflows/ingestion_workflow.py).

```python
# Part 4.b - Crawling simplified - START
sources = await crawling_stage(sources)
sources, err_sources = split_sources_by_error(sources)
sources_with_errors.extend(err_sources)
# Part 4.b - Crawling simplified - END
```

Nous verrons dans la derni√®re partie que crawler beaucoup d'information revient √† d√©clencher beaucoups d'activit√©s et
peut vite saturer l'Event History d'un run de workflow Temporal ([voir limite de m√©moire ici des payload entr√©e / sortie](https://docs.temporal.io/workflow-execution/limits#workflow-execution-limits)).

A ce moment l√† il est possible de d√©couper l'ex√©cution
du workflow en plusieurs via [Continue As New](https://docs.temporal.io/develop/php/continue-as-new).

## Tester l'ensemble

Lancer le workflow via :
* PyCharm : `Part 4.b - Crawling`
* VS Code : `[üêç] Trigger - Part 4.b - Crawling`

Vous devriez avoir un workflow beaucoup plus fourni :
![](./images/part4-Crawling_workflow_event_history.png)

N'h√©sitez pas √† aller jouer avec ces nouvelles donn√©es sur AvelBot.

---

Partie suivante : [ Partie 5 -√âviter les prompts qui explosent, d√©coupons nos documents - Chunking](./part5-chunking.md)
