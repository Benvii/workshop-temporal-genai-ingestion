# Partie 4 - Crawling et scraping des pages

* ‚è∞ Dur√©e : 25min
* üéØ Objectifs :
  * Cr√©ation de l‚Äôactivit√© de Scraping qui depuis une source avec URI va t√©l√©charger la page.
  * Appeler l'activit√© de crawling en amont
  * Comprendre le concept de Continue As New

TODO TOC

## Cr√©ation de l'activit√© de Scraping

Ajoutons d'abord une activit√© qui depuis l'URL d'une page web dont on sait qu'il s'agit d'une page HTML (mime_type: text/html)
est capable la t√©l√©charger dans le dossier de travail temporal.

![](./images/workflow-pipeline-part4.excalidraw.png)

Arborescence du dossier que nous souhaitons avoir :
```
temporal_workdir/
‚îÇ
‚îî‚îÄ‚îÄ {uuid-workflow}/
    ‚îÇ
    ‚îú‚îÄ‚îÄ scraping/                 # Pages HTML brutes r√©cup√©r√©es, en fin de stage de scraping
    ‚îÇ   ‚îú‚îÄ‚îÄ brest.fr_actualites_palaren-un-belvedere-sur-rade-en-preparation.html
    ‚îÇ   ‚îî‚îÄ‚îÄ brest.fr_actualites_debut-des-travaux-pour-les-ascenseurs-inclines-du-chu.html
    ‚îÇ
    ‚îî‚îÄ‚îÄ conversion/               # Fichiers Markdown convertis, en fin de stage de convertion
        ‚îú‚îÄ‚îÄ brest.fr_actualites_palaren-un-belvedere-sur-rade-en-preparation.html.md
        ‚îî‚îÄ‚îÄ brest.fr_actualites_debut-des-travaux-pour-les-ascenseurs-inclines-du-chu.html.md
```

Vous pourrez utiliser la librairie requests pour t√©l√©charger les fichiers a vous de jouer et de compl√©ter le fichier
[scraping_activity.py](../avelbot-ingestion-py/src/avelbot_ingestion/activities/scraping_activity.py) en vous inspirant
par exemple de [print_source_activity.py](../avelbot-ingestion-py/src/avelbot_ingestion/activities/print_source_activity.py).

Le cadre et le suivant :
* L'activit√© s'appelle `PY-scraping_activity`.
* Elle prend en entr√©e une source.
* Elle doit s'assurer que le dossier `scraping` existe en sachant que le worker s'ex√©cute depuis la racine du projet git,
donc que `./temporal_workdir` pointe bien vers le dossier de travail de Temporal.
  * Vous pouvez acc√©der √† l'UUID du workflow courant en utilisant
```python
from temporalio import activity

workflow_id = activity.info().workflow_id
```
* V√©rifier que la source est bien de type `MimeTypesEnum.text_html`, sinon la marquer en erreur.
* Vous pouvez utiliser requests pour t√©l√©charger en GET les sources.
* Vous pouvez vous aider de TODO fonction utilitaire pour g√©n√©rer un nom normalis√© depuis une URI :
```python
from avelbot_ingestion.helpers.url_helpers import url_to_file_name

url_to_file_name(source.uri)
```
* ‚ö† N'oubliez pas toutes nouvelle activit√© doit √™tre r√©f√©renc√©e sur un worker üòâ
* Vous pouvez tester √† l'aide de la run configuration PyCharm `Part 4.a - Scraping`

Si besoin la solution ci-dessous ‚¨áÔ∏è.

<details>
  <summary>Solution scraping_activity.py</summary>

```python
# COMPLETER ICI - START (partie 4)
import os
from temporalio import activity

from avelbot_ingestion.helpers.logging_config import get_app_logger
from avelbot_ingestion.helpers.url_helpers import url_to_file_name
from avelbot_ingestion.models.MimeTypesEnum import MimeTypesEnum
from avelbot_ingestion.models.Source import Source

import requests
from pathlib import Path

from avelbot_ingestion.models.StageEnum import StageEnum

logger = get_app_logger(__name__)

@activity.defn(name="PY-scraping_activity")
async def scraping_activity(source: Source) -> Source:
    """
    Activit√©e qui t√©l√©charge une page web.

    :param source: Source a t√©l√©charger.
    """
    logger.info(" ---- scraping_activity for %s ----", source.uri)

    if source.mime_type != MimeTypesEnum.text_html:
        source.error = "Not and HTML mime type, can't be scraped."
        return source

    source.current_stage = StageEnum.SCRAPING

    # Ensure output directory exists
    workflow_id = activity.info().workflow_id
    scraping_directory = Path("temporal_workdir") / workflow_id / "scraping"
    os.makedirs(scraping_directory, exist_ok=True)
    output_raw_file_path = Path(scraping_directory) / f"{url_to_file_name(source.uri)}.html"

    # Download page
    try:
        resp = requests.get(source.uri, timeout=10)
        resp.raise_for_status()

        output_raw_file_path.write_text(resp.text, encoding="utf-8")
        source.raw_file_path = str(output_raw_file_path)
    except Exception as e:
        source.error = f"Failed to download page: {e}"

    return  source

# COMPLETER ICI - END (partie 4)
```
</details>

## Appel de l'activit√© de Crawling

TODO, dev cette activit√© :
* a base de beautiful soup.
* avec pagination
* introduire en option le Continue As New ? + bulk size ?

## Tester l'ensemble

TODO

Partie suivant : Chunking TODO.
