# Partie 5 - √âviter les prompts qui explosent, d√©coupons nos documents - Chunking

* ‚è∞ Dur√©e : 25min
* üéØ Objectifs :
  * Pourquoi chunker ? probl√©matique et approches.
  * Adaptation de l‚Äôactivit√© d‚Äôindexing pour qu'elle ing√®re le chunks g√©n√©r√©s.
  * Voir l'impact sur le prompt

## Pourquoi chunker ? probl√©matique et approches

<!-- Explication √† l'oral -->
Le **chunking** est essentiel en RAG parce qu‚Äôun document brut (PDF, page web, note interne‚Ä¶) est souvent trop long ou trop dense pour √™tre utilis√© directement par un mod√®le d‚ÄôIA. Les mod√®les d‚Äôembeddings ont aussi une **fen√™tre de contexte limit√©e** : ils ne peuvent encoder efficacement qu‚Äôun certain nombre de tokens √† la fois. En d√©coupant un document en **morceaux coh√©rents et de taille ma√Ætris√©e**, on garantit que chaque chunk contient une id√©e claire, encodable proprement, avec un embedding repr√©sentatif. Sans chunking, un embedding couvrirait un √©norme bloc d‚Äôinformation h√©t√©rog√®ne, perdant en pr√©cision et rendant la r√©cup√©ration beaucoup moins pertinente.

Le chunking am√©liore √©galement la **qualit√© de la recherche** dans le RAG. Lorsqu‚Äôune question est pos√©e, le moteur vectoriel recherche les chunks les plus proches s√©mantiquement. Si les documents ne sont pas d√©coup√©s, la similarit√© serait calcul√©e sur des blocs gigantesques, entra√Ænant des r√©ponses trop g√©n√©riques ou une dilution de l‚Äôinformation utile. Avec des chunks bien calibr√©s, le syst√®me peut retrouver exactement le passage pertinent ‚Äî un paragraphe technique, une r√®gle m√©tier, un extrait juridique ‚Äî plut√¥t que tout un chapitre inutile.

Enfin, le chunking permet un **contr√¥le fin du contexte inject√© au mod√®le**. On peut √©viter de surcharger l‚Äôappel LLM avec des centaines de pages, r√©duire les co√ªts, et minimiser les risques d‚Äôerreurs (hallucinations, contradictions entre parties du document). Le chunking sert donc √† maximiser la **pr√©cision, la pertinence et l‚Äôefficacit√©** du pipeline RAG, tout en am√©liorant la robustesse du syst√®me.

Il existe diff√©rentes approches de chunking les plus connues sont, je vous invite √† voir ce thread :
[5 Chunking Techniques](https://www.threads.com/@weaviate.io/post/DAIvRkKq_ky) :

### Recursive Character Text Splitting (approche ‚Äúcascade‚Äù de LangChain)

D√©coupe en privil√©giant les s√©parateurs : d‚Äôabord les titres ‚Üí paragraphes ‚Üí phrases ‚Üí caract√®res.
* ‚úîÔ∏è Maintient des chunks coh√©rents tout en respectant une taille max
* ‚ùå Peut √™tre moins pertinent que du pur chunking s√©mantique

Je vous invite √† tester [ce playground](https://chunkviz.up.railway.app/) pour avoir une id√©e du r√©sultat :
![](./images/part45-chunkviz.png)

### Chunking s√©mantique (Semantic similarity chunking)
Utilise un mod√®le (embeddings ou LLM) pour d√©tecter les transitions s√©mantiques et regrouper les phrases qui parlent du m√™me sujet.
* ‚úîÔ∏è Chunks tr√®s pertinents et coh√©rents
* ‚ùå Plus co√ªteux et complexe

### Dense X Retrieval (un exemple de chunking par LLM)

D√©coupe le document en petits passages denses (souvent 100‚Äì200 tokens), optimis√©s pour cr√©er des embeddings tr√®s pr√©cis. Chaque passage est ensuite index√© dans un moteur de recherche dense (vector store).
* ‚úîÔ∏è Tr√®s performant pour le matching s√©mantique fin, id√©al pour les questions pr√©cises
* ‚ùå Requiert plus de chunks ‚Üí plus d‚Äôindexation et plus de stockage

![](./images/part4-chunk-denseXretreival.png)

Article int√©ressant sur le sujet [E14 : Dense X Retrieval](https://medium.com/papers-i-found/e14-dense-x-retrieval-d340d20188d3)

## Adaptation de l‚Äôactivit√© d‚Äôindexing pour qu'elle ing√®re le chunks g√©n√©r√©s

### Mod√®le de donn√©es et stockage des chunks

Nous allons utiliser le Recursive Text Splitter et cr√©er une activit√©e qui split le markdown de chaques sources en en liste
de chunks.

```mermaid
---
title: Mod√®le de donn√©es - Workflow d'ingestion
---
classDiagram
    class Chunk{
        +String text
        +Dict metadata
    }
```

Nous allons stocker les chunks dans chaques sources dans un fichier yaml localis√© comme suit : 
```
temporal_workdir/
‚îÇ
‚îî‚îÄ‚îÄ {uuid-workflow}/
    ‚îÇ
    ‚îú‚îÄ‚îÄ scraping/                   # Pages HTML brutes r√©cup√©r√©es, en fin de stage de scraping
    ‚îÇ   ‚îú‚îÄ‚îÄ brest.fr_actualites_palaren-un-belvedere-sur-rade-en-preparation.html
    ‚îÇ   ‚îî‚îÄ‚îÄ brest.fr_actualites_debut-des-travaux-pour-les-ascenseurs-inclines-du-chu.html
    ‚îÇ
    ‚îú‚îÄ‚îÄ conversion/                 # Fichiers Markdown convertis, en fin de stage de conversion
    ‚îÇ   ‚îú‚îÄ‚îÄ brest.fr_actualites_palaren-un-belvedere-sur-rade-en-preparation.html.md
    ‚îÇ   ‚îî‚îÄ‚îÄ brest.fr_actualites_debut-des-travaux-pour-les-ascenseurs-inclines-du-chu.html.md
    ‚îÇ
    ‚îî‚îÄ‚îÄ chunking/                   # R√©sultat du d√©coupage en chunks
        ‚îú‚îÄ‚îÄ brest.fr_actualites_palaren-un-belvedere-sur-rade-en-preparation__chunks.yaml
        ‚îî‚îÄ‚îÄ brest.fr_actualites_palaren-un-belvedere-sur-rade-en-preparation__chunks.yaml
```

Exemple de fichier :
```yaml
- text: "*   [Aller au contenu](#MainWrapper)\n\n [![Brest m√©tropole et ville (aller\
    \ √† l'accueil)](/sites/default/files/medias/brestfr/images/accueil/Groupe%2029.svg)![Brest\
    \ m√©tropole et ville (aller √† l'accueil)](/sites/default/files/medias/brestfr/images/accueil/Groupe%2029.svg)](/)"
  metadata:
    title: D√©but des travaux pour les ascenseurs inclin√©s du CHU
- text: "*   [Recherche](/rechercher)\n    \n*   [.st0{fill:#3A73DC;} D√©marches](/gerer-mon-quotidien/mes-demarches)\n\
    *   Menu\n    \n    *   G√©rer mon quotidien\n        \n        Retour\n      \
    \  \n        *   [D√©marches](/gerer-mon-quotidien/mes-demarches)\n        *  \
    \ Petite enfance\n            \n            Retour\n            \n           \
    \ *   [Relais petite enfance : un guichet unique pour les parents](/gerer-mon-quotidien/petite-enfance/les-relais-petite-enfance-un-guichet-unique-pour-les-parents-et)\n\
    \            *   [Accueillir un enfant : avant et apr√®s la naissance](/gerer-mon-quotidien/petite-enfance/accueillir-un-enfant-avant-et-apres-la-naissance-ladoption)"
  metadata:
    title: D√©but des travaux pour les ascenseurs inclin√©s du CHU
```

### Compl√©ter l'activit√©e

On va compl√©ter l'activit√©e de chunking text recursif, ouvrez le fichier [recursive_text_chunking_source_activity.py](../avelbot-ingestion-py/src/avelbot_ingestion/activities/recursive_text_chunking_source_activity.py) :
* Lire le contenu markdown de la source `source.converted_md_path` avec `aiofiles` : 
```python
async with aiofiles.open(source.converted_md_path, mode="r", encoding="utf-8") as src_file:
    source_content = await src_file.read()
```
* Cr√©er le splitter langchain RecursiveCharacterTextSplitter √† l'aide des param√®tres pr√©sent dans l'input du workflow, je vous invite √† voir le fichier [part5-chunking_recursive.yml](../avelbot-ingestion-py/ressources/workflow_inputs/part5-chunking_recursive.yml) qui sera utilis√© et contient ces param√®tres :
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=recursive_chunking_stage_config.chunk_size,
    chunk_overlap=recursive_chunking_stage_config.chunk_overlap,
)
chunks_texts = text_splitter.split_text(source_content)
```
* Les convertir en Chunk de notre mod√®le en reportant bien les metadata de la source : 
```python
for text in chunks_texts:
    chunks.append(Chunk(text=text, metadata=source.metadata))
```
* Sauvegarder le tout dans le fichier dans le chemin est disponible via `source_chunks_file_path`, penser √† la renseigner sur la source :
```python
await save_chunks_yaml(chunks, source_chunks_file_path)
    source.chunking_file_path = str(source_chunks_file_path)
```

<details>
 <summary>Block complet solution `recursive_text_chunking_source_activity.py` </summary>

```python
# COMPLETER ICI - START (partie 5)
async with aiofiles.open(source.converted_md_path, mode="r", encoding="utf-8") as src_file:
    source_content = await src_file.read()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=recursive_chunking_stage_config.chunk_size,
    chunk_overlap=recursive_chunking_stage_config.chunk_overlap,
)
chunks_texts = text_splitter.split_text(source_content)

for text in chunks_texts:
    chunks.append(Chunk(text=text, metadata=source.metadata))

await save_chunks_yaml(chunks, source_chunks_file_path)
source.chunking_file_path = str(source_chunks_file_path)
# COMPLETER ICI - END (partie 5)
```
</details>

### Appeler l'activit√©e depuis le workflow

Cette activit√©e n'est pas appel√©e depuis le workflow pour l'instant, le code est d√©j√† pr√®s il vous suffit de d√©commenter.

Ouvrez le fichier [ingestion_workflow.py](../avelbot-ingestion-py/src/avelbot_ingestion/workflows/ingestion_workflow.py),
d√©commentez la partie 5 :
```python
# COMPLETER ICI - START (partie 5)
# D√©commenter les lignes suivante
sources = await recursive_text_chunking_stage(sources, ingestion_workflow_input.recursive_chunking_config)
sources, err_sources = split_sources_by_error(sources)
sources_with_errors.extend(err_sources)
# Penser √† changer l'activit√© d'indexing "index_source_no_chunk_activity" par "index_source_with_chunks_activity"
# COMPLETER ICI - END (partie 5)
```

### Replacement de l'activit√© d'indexation dans le workflow

Nous allons appeler une activit√©e d'indexation qui se base sur ce fichier de chunks pour avoir en base 1 document par chunk
est plus 1 document avec le contenu complet de la source.
Cette activit√© est situ√©e ici [index_source_with_chunks_activity.py](../avelbot-ingestion-py/src/avelbot_ingestion/activities/index_source_with_chunks_activity.py),
elle s'appelle `index_source_with_chunks_activity`.

Vous devez adapter le workflow [ingestion_workflow.py](../avelbot-ingestion-py/src/avelbot_ingestion/workflows/ingestion_workflow.py)
pour qu'elle appel cette activit√© √† la place de l'ancienne. Comme ci-dessous :

```python
indexing_tasks = [
    workflow.execute_activity(
        activity=index_source_with_chunks_activity,
        task_queue="PY_WORKER_TASK_QUEUE",
        args=[source, ingestion_workflow_input.indexing_config], # Ne pas oublier de passer le param√®tre suppl√©mentaire ici
        start_to_close_timeout=timedelta(seconds=WORKFLOW_ACTIVITY_START_TO_CLOSE_TIMEOUT),
    )
    for source in sources
]
```

### Bien s'assurer que le worker g√®res ces activit√©s

‚ö†Ô∏è Bien penser √† ajouter les 2 activit√©s aux Main Worker dans [main_worker.py](../avelbot-ingestion-py/src/avelbot_ingestion/worker/main_worker.py) :
```python
worker: Worker = Worker(
        client,
        task_queue="PY_WORKER_TASK_QUEUE", # Nom de la file sur laquelle √©coute le worker
        workflows=[IngestionWorkflow], # Code des workflow qu'est capable d'ex√©cuter le worker
        activities=[print_source_activity, # Liste des activit√©s qu'est capable d'ex√©cuter le worker
                    index_source_no_chunk_activity,
                    scraping_activity,
                    crawling_activity,
                    recursive_text_chunking_source_activity, # Ajout√© ici !!
                    index_source_with_chunks_activity # Ajout√© ici !!
                    ],
        debug_mode=True,
        workflow_runner=build_sandbox_worker_runner_vscode_debug_compatible() # Used to prevent VS Code issue
    )
```

### Lancer le workflow

Nous allons partir des 2 articles d√©j√† crawl√©s.
* PyCharm `Part 5 - Chunking Rec (2 articles)` bas√©e sur [part5-chunking_recursive.yml](../avelbot-ingestion-py/ressources/workflow_inputs/part5-chunking_recursive.yml) 
* VS Code `[üêç] Trigger - Part 5 - Chunking Rec (2 articles)`
* En terminal : 
```bash
cd $(git rev-parse --show-toplevel)/avelbot-ingestion-py/ressources/workflow_inputs
python ../../src/avelbot_ingestion/runners/trigger_ingestion_workflow.py --workflow-input-yaml=part5-chunking_recursive.yml
```

## Voir l'impact sur le prompt

Vous devriez avoir le workflow suivant :
![](./images/part5-workflow_chunking_temporalui.png)

Retournez voir le nombre de documents dans votre base vectorielle et interroger le bot pour voir beaucoup plus de
documents et dans prompt plus l√©ger dans les traces langfuse.

Vous devriez avoir **~100 documents** :
![](./images/part5-avelbot-nb_documents.png)


Partie suivante : PARTIE FINALE TODO